{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxmPN3s2j2Gzoj2igkXXx/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1ucky40nc3/serve_mpt/blob/main/examples/notebooks/run_quant_mpt_with_ctransformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MPT Quantization and Inference"
      ],
      "metadata": {
        "id": "_N4E0Bt8WWgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "qKoEFXKUdrAR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjCG-pOAhsTz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install Requirements\n",
        "!pip install einops\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install sentencepiece\n",
        "!pip install ctransformers\n",
        "\n",
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Clone the ggml Repository\n",
        "!git clone https://github.com/ggerganov/ggml"
      ],
      "metadata": {
        "id": "vvxgEDkGh6w9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Build the ggml MPT Features\n",
        "!mkdir -p /content/ggml/build\n",
        "%cd /content/ggml/build\n",
        "!cmake ..\n",
        "!make -j4 mpt mpt-quantize"
      ],
      "metadata": {
        "id": "j5TM8ERNh9Nt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare a **MosaicML MPT-7B** PLM\n",
        "\n"
      ],
      "metadata": {
        "id": "AAgMtGdwdypo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download a `mosaicml/mpt-7b*` PLM from [https://huggingface.co](https://huggingface.co)\n",
        "!mkdir -p /content/models\n",
        "%cd /content/models\n",
        "\n",
        "# @markdown Select a pretrained model checkpoint:\n",
        "model_repository = \"mosaicml/mpt-7b-instruct\" # @param ['mosaicml/mpt-7b', 'mosaicml/mpt-7b-storywriter', 'mosaicml/mpt-7b-chat', 'mosaicml/mpt-7b-instruct']\n",
        "model_dir = \"_\".join(model_repository.split(\"/\")[1:])\n",
        "huggingface_base_url = \"https://huggingface.co\"\n",
        "huggingface_mpt_repository_url = f\"{huggingface_base_url}/{model_repository}\"\n",
        "\n",
        "!git clone $huggingface_mpt_repository_url"
      ],
      "metadata": {
        "id": "6hJbUHIOh_Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Import Dependencies\n",
        "import os"
      ],
      "metadata": {
        "id": "dB6jktNso3J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Convert the PLM into the ggml Format\n",
        "%cd /content/ggml/build\n",
        "\n",
        "base_dir = \"/content/models\"\n",
        "model_dir = \"mpt-7b-instruct\"\n",
        "model_path = os.path.join(base_dir, model_dir)\n",
        "\n",
        "# @markdown Select a floating point precision for your ggml model:\n",
        "ftype = \"f32\" # @param ['f32', 'f16']\n",
        "ftype_types = [\"f32\", \"f16\"]\n",
        "ftype_index = ftype_types.index(ftype)\n",
        "\n",
        "!python ../examples/mpt/convert-h5-to-ggml.py \\\n",
        "    $model_path \\\n",
        "    $ftype_index"
      ],
      "metadata": {
        "id": "MNO_i8HpiAjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Quantize the PLM\n",
        "%cd /content/ggml/build\n",
        "\n",
        "# @markdown Select a quantization config:\n",
        "quant_config = \"q4_0\" # @param ['q4_0', 'q4_1', 'q5_0', 'q5_1', 'q8_0']\n",
        "ggml_model_path = os.path.join(base_dir, model_dir, f\"ggml-model-{ftype}.bin\")\n",
        "quant_model_file = f\"{model_dir}_ggml_quant-{quant_config}.bin\"\n",
        "quant_model_path = os.path.join(base_dir, model_dir, quant_model_file)\n",
        "\n",
        "!./bin/mpt-quantize \\\n",
        "    $ggml_model_path \\\n",
        "    $quant_model_path \\\n",
        "    $quant_config"
      ],
      "metadata": {
        "id": "4dgdPH3Hthb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save the Quantized Model in your Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "# @markdown Set a target path for the quantized model:\n",
        "gdrive_path = \"/content/gdrive/MyDrive/\" # @param {\"type\": \"string\"}\n",
        "\n",
        "!cp $quant_model_path $gdrive_path"
      ],
      "metadata": {
        "id": "cbGhyDGKnF7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do some Inference!"
      ],
      "metadata": {
        "id": "eQbq9JWvVDmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Import Dependencies\n",
        "from ctransformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "A9IjB1Ow1Upb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load the *Quantized* GGML **MosaicML MPT-7B-Instruct**\n",
        "llm_quant = AutoModelForCausalLM.from_pretrained(\n",
        "    quant_model_file, \n",
        "    model_type=\"mpt\"\n",
        ")"
      ],
      "metadata": {
        "id": "r9ykV9dwvWLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Do some Inference\n",
        "llm_quant(\"AI is going to\")"
      ],
      "metadata": {
        "id": "oIulSWz6voqk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}